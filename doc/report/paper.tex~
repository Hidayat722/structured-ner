

\documentclass[11pt]{article}
\usepackage{acl2011}
\usepackage[utf8]{inputenc}
\usepackage{linguex}
\usepackage{latexsym}  
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{verbatim}
\usepackage{array}
\usepackage{subfigure}
\usepackage{hyperref}
\usepackage{amsmath}
% \usepackage{xcolor}
% \definecolor{dark-red}{rgb}{0.4,0.15,0.15}
% \definecolor{dark-blue}{rgb}{0.15,0.15,0.4}
% \definecolor{medium-blue}{rgb}{0,0,0.5}
% \hypersetup{
%     colorlinks, linkcolor={dark-red},
%     citecolor={dark-blue}, urlcolor={medium-blue}}
\title{Named Entity Recognition using Structured Prediction}

% \author{Joachim Daiber \\
%   %Affiliation / Address line 1 \\
%  
%   {\tt email@domain} \\\And
%   Carmen Klaussner \\
%   %Affiliation / Address line 1 \\
% 
%   {\tt carmen@wordsmith.de} \\
%   }
% 
% \date{today}


\author{Joachim Daiber \\
  University of Groningen \\
  %Affiliation / Address line 2 \\
  %Affiliation / Address line 3 \\
  {\tt jodaiber@gmx.de} \\\And
  Carmen Klaussner \\
  University of Groningen \\
  %Affiliation / Address line 2 \\
  %Affiliation / Address line 3 \\
  {\tt Carmen@wordsmith.de} \\}

\date{}

\begin{document}
\maketitle

\section{Introduction}
Named Entity Recognition (NER) is an information extraction task and has first been introduced as part of the 6th MUC (Sixth Message Understanding Conference)
that was focusing on the extraction of structured information from unstructured text, such as company names from newspaper articles \cite{nadeau2007survey}.
The most prominent types are generally known under the name of \emph{enamex} types, which are comprised of \texttt{Person Names}, \texttt{Organisations} and \texttt{Locations}. 
An additional type \texttt{Miscellaneous} captures person names outside the classic \emph{enamax} type.
Apart from these there is \emph{timex}, which covers date \& time expressions and \emph{numex} which is for monetary values \& percent. 
Table \ref{table:NETypes} gives an overview of the different NE types.

\begin{table}[h!]
\scriptsize
\begin{tabular}{l|l}
\hline
\bf Named Entity Type & \bf Example \\
\hline
Person Names & Greta Garbo, John, Mary \\
Organisations& Benetton, Coca-Cola Gmbh\\
Locations&  Paris, Australia, Bristol\\
Miscellaneous& World Cup\\
 Date Expression& 01-02-2012, January, 6th, 1998 \\
Time Expression & 10 p.m.\\
Monetary Value &  \$15, 100 Euro   \\
Percent &   30\%
\end{tabular}
\caption{NE Examples}
\label{table:NETypes}
\end{table}


In considering named entities, it is important to distinguish between mention of entities that are non - specific as 
time expressions, such as \emph{In June}, referring to any possible year or \emph{the prof} as a person name , which itself is not a specific entity, but a deictic reference
, which may point back to the mention of a real \emph{NE}, as in the following coreference chain: \\


\ex. $ Prof.\; Bateman \Rightarrow he \Rightarrow the \; prof$


\subsection*{Issues in Named Entity Recognition}
Among the most common problems for NER is the issue of ambiguity, that is in particular \emph{Polysemy} \cite{nadeau2007survey}, 
the property of some lexical representation having
more than one possible meaning, and \emph{Metonymy}, which refers to the concept of the part-whole/whole-part relation between two expressions. 
\emph{Polysemy} could become an issue for NER when the lexical representation of an item could point to two different \emph{NE} types.
This is quite frequent with \texttt{Person Names} and \texttt{Locations}, since many people are named after cities, such as \emph{Paris} or \emph{Georgia}. 
Often the context will not be disambiguating as in \ref{ex:Paris}.

\ex. \emph{Paris is beautiful.} \label{ex:Paris}

\emph{Metonymy} is frequently an issue in literary texts and news data (which is often used in NER), where two items that are in a part-whole
relationship, are substituted for each other respectively. Example \ref{ex:Lon} shows an instance of \emph{whole-part}, where \emph{London} is supposedly substituted
for the \textbf{Goverment in London}. 

\ex. \emph{\textbf{London} decided to increase the 1200 military personnel involved in Olympic security.} \label{ex:Lon}

\subsection*{Methods employed for NER}
For \emph{NER}, there exist both rule-based and statisical approaches. 
Rule-based methods make use of the underlying rules governing languages to extract named entities. 
However, this approach is high maintenance, quite time-consuming and requires extensive work of computational linguists \cite{nadeau2007survey}
and although the results are 
often high in precision, lacks considerably in recall.

In regard to statistical approaches, supervised-learning is the most common method applied in \emph{NER}. Although, there are unsupervised approaches, 
their performance is not as high as for SL applications yet.

Prominent algorithms in NER include maximum entropy
% Overview algorithms /methods used 
% machine learning/neural networks/rule-based 
% maximum entropy MM
% 



\subsection*{CoNLL data set \& baseline}
The training and test dataset was taken from the \emph{CoNLL Shared Task 2002} \cite{TjongKimSang:2003:ICS:1119176.1119195} and the \emph{CoNLL Shared Task 2003} \cite{tksintro2002conll}
for Spanish \& Dutch and English \& German respectively.

% A baseline rate was computed for the English and the
% German test sets. It was produced by a system which
% only identified entities which had a unique class in
% the training data. If a phrase was part of more than
% one entity, the system would select the longest one.
% All systems that participated in the shared task have
% outperformed the baseline system.
% For all the Fβ=1 rates we have estimated sig-
% nificance boundaries by using bootstrap resampling
% (Noreen, 1989). From each output file of a system,
% 250 random samples of sentences have been chosen
% and the distribution of the Fβ=1 rates in these sam-
% ples is assumed to be the distribution of the perfor-
% mance of the system. We assume that performance
% A is significantly different from performance B if A
% is not within the center 90% of the distribution of B.

The baseline for \emph{Named Entity Recognition} in the four different languages is visualised in table \
\begin{table}[h!]
\scriptsize
\begin{tabular}{l|l|l|l}
\hline
\bf Language & \bf Precision & \bf Recall & \bf $F_1$-measure \\ \hline
Spanish &             26.27 \% & 56.48 \% & 35.86  \%        \\
Dutch  &             64.38 \%  &45.19   \%    & 53.10 \%  \\
English &              71.91\%& 50.90 \%  & 59.61 $\pm$ 1.2 \%\\
German &      31.86 \%  & 28.89 \% & 30.30  $\pm$ \%1.3 \\
\end{tabular}
\caption{NER Baseline}
\label{table:Base}
\end{table}

\section{Structured Prediction}
Structured Prediction \cite{strlearn} 
is a supervised-learning approach and sets itself apart from Non-structured Prediction through the form of its output. 
Prediction maps an input x to an output y: \\
$ x \rightarrow y $. 

Non-structured output is atomic, thus it is binary prediction for a two-class problem and may corresponds to more than one of more than 2 possible labels for a 
multiclass problem. 
The output of Structured Prediction is structured and gives back a sequence/tree. 
Thus, the output could be the complete sentence with its corresponding labels. 

\ex. \emph{The motor company 'Ford' was founded and incorporated  by Henry Ford on June, 16th 1903.} \label{PredEx1a}

would trigger the labels: 
\ex. x The motor company 'Ford' was founded and incorporated  by Henry Ford on June, 16th 1903. \label{PredEx1b} % include labels 
     %y 
Furthermore, Structured Prediction is different from similar approaches such as HMM through the combination of features and the use of label interactions.


\section{Our structured Perceptron}
Our NER structured perceptron for the languages English, German, Dutch and Spanish is trained and tested on the \emph{CoNLL 2003} and \emph{CoNLL 2002} data sets
respectively. 

\paragraph*{Learning}
Labels of the whole sentence (0) for zero entity
\paragraph*{Structure}
Structured Perceptron with Averaging

\paragraph*{Decoding}
Viterbi algorithm (Markov assumption, only 1 prev.
label)

\subsection*{Features}
The features employed in the system can be divided into three categories: node, label and gazetteer features. 
We describe each of the three groups in the following.

\paragraph*{Node features}
These are only present on the word in question: the \emph{Token}, suffix and prefix, capitalisation.

Table \ref{table:node} shows the various node features with an example respectively.

\begin{table}[h!]
\begin{tabular}{l|l}
\hline
\bf Feature & \bf Example \\
\hline
Token &  \\
Suffix& Amster\textbf{dam}\\
Prefix&  \textbf{San} Sebastian\\
Captitalized& \textbf{B}enetton\\
Number Pattern & \\
UPPERCASE &  \textbf{BENETTON}\\
POS-tag &  Benetton \textbf{NNP}   \\
Lemma &   \\
\end{tabular}
\caption{Node Features}
\label{table:node}
\end{table}

\paragraph*{Label Interaction Features}
These features register which label has been assigned to the previous token and takes into account the most likely sequence. 
%current token and last label for prepositions or possessive ’s


\paragraph*{Gazetteer Features}
In order to create gazetteer lists for the more common named entities, we designed a \emph{SPARQL} query, that would retrieve entries from \emph{DBPedia} for all
languages. The reliability of the respective list is learnt be the perceptron. 


\section{Experiments/ Evaluation}
In the following section we present our experiments and the evalution of our system.
\subsection*{Experiments}


\subsection*{Evaluation}
For the evaluation of the system we used \emph{Precision} and \emph{Recall} as shown in \ref{Precision} and \ref{Recall} respectively.
The general formula for the \emph{F-Score} is shown in \ref{Fscore}. Since we rate both \emph{Precision} and \emph{Recall} evenly, we 
use the harmonic mean as shown in \ref{F1}.


\ex. \emph{Precision} = $ \frac{gold\; tag \bigcap predicted}{predicted}$ \label{Precision}\\


\ex. \emph{Recall} = $ \frac{gold \;tag \bigcap predicted}{gold\; tag}$ \label{Recall}\\


\ex. $F_{\beta}$ = $ (1+\beta^2)*\frac{precision *recall}{\beta^2* precision + recall}$ \label{Fscore}\\

\ex. $F_1$ = $ 2*\frac{precision *recall}{precision + recall}$ \label{F1}\\




\begin{table}[h!]
\begin{tabular}{l |l| l| l| l| l| l}
 \hline 
\bf Language & \multicolumn{3}{c|}{ \bf testA}&\multicolumn{3}{c|}{ \bf testB}\\\hline 
             & Precision & Recall & $F_1$-measure & Precision & Recall & $F_1$-measure \\ \hline
Spanish &       &          &     &          &               & \\
Dutch  &         &          &     &          &               &   \\
English &        &          &     &          &               &       \\
German &      &          &       &          &             & \\
\end{tabular}
\caption{NER Structured Prediction Results }
\label{table:Results}
\end{table}

\section{Discussion of Results}

\subsection*{Some Challenges} % maybe in previous section


\section{Future Work \& Conclusion}




\bibliographystyle{acl}
\bibliography{paper}

\end{document}
