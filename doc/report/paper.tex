

\documentclass[11pt]{article}
\usepackage{acl2011}
\usepackage[utf8]{inputenc}
\usepackage{linguex}
\usepackage{latexsym}  
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{verbatim}
\usepackage{array}
\usepackage{subfigure}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{qtree}
%\usepackage{gb4e}

%\usepackage{gb4e}


% \usepackage{xcolor}
% \definecolor{dark-red}{rgb}{0.4,0.15,0.15}
% \definecolor{dark-blue}{rgb}{0.15,0.15,0.4}
% \definecolor{medium-blue}{rgb}{0,0,0.5}
% \hypersetup{
%     colorlinks, linkcolor={dark-red},
%     citecolor={dark-blue}, urlcolor={medium-blue}}
\title{Named Entity Recognition with Structured Prediction}

% \author{Joachim Daiber \\
%   %Affiliation / Address line 1 \\
%  
%   {\tt email@domain} \\\And
%   Carmen Klaussner \\
%   %Affiliation / Address line 1 \\
% 
%   {\tt carmen@wordsmith.de} \\
%   }
% 
% \date{today}


\author{Joachim Daiber \\
  University of Groningen \\
  %Affiliation / Address line 2 \\
  %Affiliation / Address line 3 \\
  {\tt jodaiber@gmx.de} \\\And
  Carmen Klaussner \\
  University of Groningen \\
  %Affiliation / Address line 2 \\
  %Affiliation / Address line 3 \\
  {\tt Carmen@wordsmith.de} \\}

\date{}

\DeclareMathOperator*{\argmax}{arg\,max}

\newcommand{\namedentity}{Named Entity} 
\newcommand{\Oo}{\texttt O} 


\begin{document}
\maketitle

\section{Introduction}
Named Entity Recognition (NER) is an information extraction task and has first been introduced as part of the 6th MUC (Sixth Message Understanding Conference)
that was focusing on the extraction of structured information from unstructured text, such as company names from newspaper articles \cite{nadeau2007survey}.
The most prominent types are generally known under the name of \emph{enamex} types, which are comprised of \texttt{Person Names}, \texttt{Organisations} and \texttt{Locations}. 
An additional type \texttt{Miscellaneous} captures names outside the classic \emph{enamax} type.
Apart from these there is \emph{timex}, which covers date \& time expressions and \emph{numex} which is for monetary values \& percent. 
Table \ref{table:NETypes} gives an overview of the different NE types.

\begin{table}[h!]
\scriptsize
\begin{tabular}{| l | l |}
\hline
\bf Named Entity Type & \bf Example \\
\hline
Person Names & Greta Garbo, John, Mary \\
Organisations& Benetton, Coca-Cola Gmbh\\
Locations&  Paris, Australia, Bristol\\
Miscellaneous& World Cup\\
 Date Expression& 01-02-2012, January, 6th, 1998 \\
Time Expression & 10 p.m.\\
Monetary Value &  \$15, 100 Euro   \\
Percent &   30\% \\
\hline
\end{tabular}
\caption{NE Examples}
\label{table:NETypes}
\end{table}


In considering named entities, it is important to distinguish between mention of entities that are non-specific as 
time expressions, such as \emph{In June}, referring to any possible year or \emph{the prof} as a person name, which itself is not a specific entity, 
but a deictic reference, which may point back to the mention of a real \emph{NE}, as in the following coreference chain: \\


\ex. $ Prof.\; Bateman \Rightarrow he \Rightarrow the \; prof$ \\

Named Entities can consist of more than one syntactic type. The usual overall type for a named entity is noun phrase, which can be both simple and complex. 
An example for a simple heterogenous entity is displayed in syntactic tree represenation in ~\ref{tree1}.
 \Tree
    [.NP [.N' John ]  ] \label{tree1}

Most languages allow for a recursive definition of, for instance, a noun phrase that in theory can grow infinitely big. 
An \textbf{NP} can consist of a determiner, such as \emph{the} and a common noun or simply a propernoun. \textbf{PP} stands
for prepositional phrase and consists of a preposition and a noun phrase. 
The syntactic rules for PP-attachment are displayed in \ref{PP}.
\begin{figure}
$ NP \rightarrow NP PP$ \\
$ PP \rightarrow P NP$ 
\caption{Recursive Noun Phrase}
\label{PP}
\end{figure}   
These rules allow for constructions such as in sentence~\ref{PPrec}. 

\ex. The princess of Scotland is in the castle in England with the brown bricks on its roof... \label{PPrec}

We distinguish between PP-complements and PP-modifiers, where modifiers simply add to the description of the noun phrase, maybe
to set it apart and complements are more inherent to the meaning of the entity. 
The PP-attachment \emph{of Scotland} is semantically defining for the noun \emph{princess}, whereas \emph{with the brown bricks on its roof} 
just adds to the description of \emph{the castle in England}.
Regarding named entities, one wants to differentiate between a PP-attachment that defines the entity an one that is irrelevant. 


\begin{enumerate}
\item \parbox[t]{2.4in}{ (a)~\Tree 
   [.NP [ [.DT the ] [.N\1 [.N\1 Princess ] [.PP [.P of ] [.NP England ] ] ] ] ]}
\parbox[t]{2in}{ (b)~\Tree 
   [.NP  [.DT the ] [.N\1 [.N\1 Princess ] ] [.PP [.P from ] [.NP England ] ]  ] ]  }
\end{enumerate}
    



\subsection*{Issues in Named Entity Recognition}
Among the most common problems for NER is the issue of ambiguity, that is in particular \emph{Polysemy} \cite{nadeau2007survey}, 
the property of some lexical representation having
more than one possible meaning, and \emph{Metonymy}, which refers to the concept of the part-whole/whole-part relation between two expressions. 
\emph{Polysemy} could become an issue for NER when the lexical representation of an item could point to two different \emph{NE} types.
This is quite frequent with \texttt{Person Names} and \texttt{Locations}, since many people are named after cities, such as \emph{Paris} or \emph{Georgia}. 
Often the context will not be disambiguating as in \ref{ex:Paris}.

\ex. \emph{Paris is beautiful.} \label{ex:Paris}

\emph{Metonymy} is frequently an issue in literary texts and news data (which is often used in NER), where two items that are in a part-whole
relationship, are substituted for each other respectively. Example \ref{ex:Lon} shows an instance of \emph{whole-part}, where \emph{London} is supposedly substituted
for the \textbf{Goverment in London}. 

\ex. \emph{\textbf{London} decided to increase the 1200 military personnel involved in Olympic security.} \label{ex:Lon}

\subsection*{Methods employed for NER}
For \emph{NER}, there exist both rule-based and statisical approaches. 
Rule-based methods make use of the underlying rules governing languages to extract named entities. 
However, this approach is high maintenance, quite time-consuming and requires extensive work of computational linguists \cite{nadeau2007survey}
and although the results are often high in precision, lacks considerably in recall.

In regard to statistical approaches, supervised-learning is the most common method applied in \emph{NER}. Although, there are unsupervised approaches, 
their performance is not as high as for SL applications yet.

Prominent algorithms in NER include maximum entropy models, 

neural network



\subsection*{CoNLL Data and Baseline Performance}
The training and test dataset was taken from the \emph{CoNLL Shared Task 2002} \cite{tksintro2002conll} and the \emph{CoNLL Shared Task 2003} \cite{TjongKimSang:2003:ICS:1119176.1119195}.
for Spanish \& Dutch and English \& German respectively. The ConNLL shared tasks are organised challenges, where the organiser propose a task and provide the
training and test data. For 2002/2003 it was Named Entity Recognition. Also we base our system on the approach that won the challenge for English and German in 2003. 
The baseline rate for all languages was produced by a system, that had a unique class in the training data. In case that a phrase was part of more than one entity, 
the system would choose the longest one \cite{TjongKimSang:2003:ICS:1119176.1119195}. 
The data is annotated with POS-tags and NE-tags, as well as Bio-tags that indicate the position in the Named Entity. 
% A baseline rate was computed for the English and the
% German test sets. It was produced by a system which
% only identified entities which had a unique class in
% the training data. If a phrase was part of more than
% one entity, the system would select the longest one.
% All systems that participated in the shared task have
% outperformed the baseline system.
% For all the Fβ=1 rates we have estimated sig-
% nificance boundaries by using bootstrap resampling
% (Noreen, 1989). From each output file of a system,
% 250 random samples of sentences have been chosen
% and the distribution of the Fβ=1 rates in these sam-
% ples is assumed to be the distribution of the perfor-
% mance of the system. We assume that performance
% A is significantly different from performance B if A
% is not within the center 90% of the distribution of B.

The baseline for \emph{Named Entity Recognition} in the four different languages is visualised in table \
\begin{table}[h!]
\scriptsize
\begin{tabular}{|l|l|l|l|}
\hline
\bf Language & \bf Precision & \bf Recall & \bf $F_1$-measure \\ \hline
Spanish &             26.27\% & 56.48\% & 35.86\%        \\
Dutch  &             64.38\%  &45.19\%    & 53.10\%  \\
English &              71.91\%& 50.90\%  & 59.61 $\pm$ 1.2\%\\
German &      31.86\%  & 28.89\% & 30.30  $\pm$ 1.3\% \\
\hline
\end{tabular}
\caption{NER Baseline}
\label{table:Base}
\end{table}

\section{Structured Prediction}
Structured Prediction \cite{strlearn} is a supervised-learning approach that maps an input \textbf{x} to an output \textbf{y}: \\

$ x \rightarrow y $. \\

Structured and  Non-structured prediction differ in the form of their output. 
Non-structured output is atomic; it is binary prediction for a two-class problem and may corresponds to more than one of more than two possible labels for a 
multiclass problem. 
Structured Prediction gives back the prediction for the whole sequence. 
For the sentence in \ref{PredEx1a}, it would return the corresponding labels combination in example~\ref{PredEx1b}. 

\ex. \emph{The motor company 'Ford' was founded and incorporated  by Henry Ford.} \label{PredEx1a}
%The motor company 'Ford' was founded and incorporated  by Henry Ford on June, 16th 1903.
 
\exg. \textbf{x:} The motor company ' Ford ' was founded and incorporated by Henry Ford .\\
      \textbf{y:} O O O O ORG  O  O O O O O PER PER O  \label{PredEx1b} \\
     
Also, Structured Prediction is different from similar approaches through its combination of features and label interactions instead
of concentrating mainly on label interactions like HMM or a rich set of features like local classifiers. 

\section{Implementation}

\subsection{Learning and Decoding}

% problem external data / use Dbpedia - gazetteer
Our implementation for the languages English, German, Dutch and Spanish is trained and tested on the \emph{CoNLL 2003} and \emph{CoNLL 2002} data sets respectively. 

As the predicted structure, we use the labels for every token in the full sentence. If a token is not a \namedentity, it is assigned the label \Oo.

Learning is implemented using the Structured Perceptron algorithm. To avoid overfitting, we average the parameters after the last iteration \cite{collins2002discriminative}. For finding the best sequence of labels, we use a modified version of the Viterbi algorithm. The algorithm finds the sequence $\hat{y}$, such that:

\[
\hat{y} = \argmax_{\mathbf{y} \in \mathcal{Y}^{n}} \sum_{i=1}^{n}\mathbf{w} \cdot \boldsymbol{f}(\mathbf{x}, i, y_{i-1}, y_{i})
\]

This sequence can be computed efficiently in $ O( N^2 |\mathbf{x}| ) $ via dynamic programming. 
As a first step, the trellis has to be constructed, then we have to find the $ a \in \mathcal{Y}$ in the 
last column of the trellis with maximal score $\delta_n(a)$. From this, the sequence can be recovered via back-tracking trough the trellis. 
The score of the best sequence ending in $a$ is:

\[
\delta_i(a) = \max_{\mathbf{y} \in \mathcal{Y}^{n}, y_n = a} \sum_{j=1}^{n}{\mathbf{w} \cdot \boldsymbol{f}(\mathbf{x}, j, y_{j-1}, y_{j})}
\]

\noindent This function $\delta_i(a)$ can be defined recursively as:

\begin{align*}
\delta_1(a) &= \mathbf{w} \cdot \boldsymbol{f}(\mathbf{x}, 1, \emptyset, a) \\
\delta_i(a) &= \max_{b \in \mathcal{Y}} \delta_{i-1}(b) + \mathbf{w} \cdot \boldsymbol{f}(\mathbf{x}, i, b, a) \\
\end{align*}


\subsection{Features}
The features employed in the system can be divided into three categories: node, label and gazetteer features. 
We describe each of the three groups in the following.

\paragraph*{Node features}
Node features are features that only depend on the current token (node): the \emph{Token}, suffix and prefix, capitalisation.

Table \ref{table:node} shows the various node features with an example respectively.

\begin{table}[h!]
\begin{tabular}{| l | l |}
\hline
\bf Feature & \bf Example \\
\hline
Token &  \textbf{``Amsterdam''}\\
Suffix& Amster\textbf{dam}\\
Prefix&  \textbf{San} Sebastian\\
Captitalized& \textbf{B}enetton\\
Number Pattern & \\
UPPERCASE &  \textbf{BENETTON}\\
POS-tag &  Benetton \textbf{NNP}   \\
Lemma &  produced $\Rightarrow$ produce \\
\hline
\end{tabular}
\caption{Node Features}
\label{table:node}
\end{table}

\paragraph*{Label Interaction Features}
These features register which label has been assigned to the previous token and takes into account the most likely sequence. 
Thus, for the sequence: ``Jack London went to New York'' the NE tag combination of \emph{PER} and \emph{PER} in example~\ref{seq1}is more likely 
than\emph{PER} and \emph{LOC} as presented in \ref{seq2}.
.
%current token and last label for prepositions or possessive ’s

\exg. Jack London went to New York .\\
      PER   PER   O    O  LOC LOC  O \\\label{seq1}

\exg. Jack London went to New York . \\ 
      PER  LOC    O    O  LOC LOC O \\\label{seq2}
    

\paragraph*{Gazetteer Features}
In order to create gazetteer lists for the more common named entities, we designed a \emph{SPARQL} query, that would retrieve entries from \emph{DBPedia} for all
languages. The reliability of the respective list is learnt be the perceptron. 


\section{Experiments/ Evaluation}
In the following section we present our experiments and the evalution of our system.
\subsection*{Experiments}


\subsection*{Evaluation}
For the evaluation of the system we used \emph{Precision} and \emph{Recall} as shown in \ref{Precision} and \ref{Recall} respectively.
The general formula for the \emph{F-Score} is shown in \ref{Fscore}. Since we rate both \emph{Precision} and \emph{Recall} evenly, we 
use the harmonic mean as shown in \ref{F1}.


\ex. \emph{Precision} = $ \frac{gold\; tag \bigcap predicted}{predicted}$ \label{Precision}\\


\ex. \emph{Recall} = $ \frac{gold \;tag \bigcap predicted}{gold\; tag}$ \label{Recall}\\


\ex. $F_{\beta}$ = $ (1+\beta^2)*\frac{precision *recall}{\beta^2* precision + recall}$ \label{Fscore}\\

\ex. $F_1$ = $ 2*\frac{precision *recall}{precision + recall}$ \label{F1}\\




\begin{table}[h!]
\scriptsize
\begin{tabular}{l |l| l| l| l| l| l}
 
\bf Language & \multicolumn{3}{c|}{ \bf testA}&\multicolumn{3}{c}{ \bf testB}\\\hline 
             & Precision & Recall & $F_1$ & Precision & Recall & $F_1$ \\ \hline
Spanish &       &          &     &          &               & \\
Dutch  &         &          &     &          &               &   \\
English &        &          &     &          &               &       \\
German &      &          &       &          &             & \\
\end{tabular}
\caption{NER Structured Prediction Results }
\label{table:Results}
\end{table}

\section{Discussion of Results}

\subsection*{Some Challenges} % maybe in previous section


\section{Future Work \& Conclusion}




\bibliographystyle{acl}
\bibliography{paper}

\end{document}
