

\documentclass[11pt]{article}
\usepackage{acl2011}
\usepackage[utf8]{inputenc}
\usepackage{linguex}
\usepackage{latexsym}  
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{verbatim}
\usepackage{array}
\usepackage{subfigure}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{qtree}
\usepackage{textcomp}

%\usepackage{gb4e}

%\usepackage{gb4e}


% \usepackage{xcolor}
% \definecolor{dark-red}{rgb}{0.4,0.15,0.15}
% \definecolor{dark-blue}{rgb}{0.15,0.15,0.4}
% \definecolor{medium-blue}{rgb}{0,0,0.5}
% \hypersetup{
%     colorlinks, linkcolor={dark-red},
%     citecolor={dark-blue}, urlcolor={medium-blue}}
\title{Named Entity Recognition as Structured Prediction}

% \author{Joachim Daiber \\
%   %Affiliation / Address line 1 \\
%  
%   {\tt email@domain} \\\And
%   Carmen Klaussner \\
%   %Affiliation / Address line 1 \\
% 
%   {\tt carmen@wordsmith.de} \\
%   }
% 
% \date{today}


\author{Joachim Daiber \\
  University of Groningen \\
  2397331\\
  %Affiliation / Address line 3 \\
  {\tt jodaiber@gmx.de} \\\And
  Carmen Klaussner \\
  University of Groningen \\
  2401541\\
  %Affiliation / Address line 3 \\
  {\tt Carmen@wordsmith.de} \\}

\date{}

\DeclareMathOperator*{\argmax}{arg\,max}

\newcommand{\namedentity}{Named Entity} 
\newcommand{\Oo}{\texttt O} 


\begin{document}
\maketitle

\begin{abstract}
Named Entity Recognition is a task

\end{abstract}


\section{Application: Named Entity Recognition}

Named Entity Recognition (NER) is an information extraction task and has first been introduced as part of the 6th MUC (Sixth Message Understanding Conference)
that was focusing on the extraction of structured information from unstructured text, such as company names from newspaper articles \cite{nadeau2007survey}.
The most prominent types are generally known under the name of \emph{enamex} types, which are comprised of \texttt{Person Names}, 
\texttt{Organisations} and \texttt{Locations}. 
An additional type \texttt{Miscellaneous} captures names outside the classic \emph{enamax} type.
Apart from the above, there is also the \emph{timex} type, which covers date and time expressions and \emph{numex} for monetary values and percentages. 
Table \ref{table:NETypes} gives an overview of the different Named Entity (NE) types.

\begin{table}[h!]
\scriptsize
\begin{tabular}{| l | l |}
\hline
\bf Named Entity Type & \bf Example \\
\hline
Person Names & Greta Garbo, John, Mary \\
Organisations& Benetton, Coca-Cola Gmbh\\
Locations&  Paris, Australia, Bristol\\
Miscellaneous& World Cup 2014, Italien\\
 Date Expression& 01-02-2012, January, 6th, 1998 \\
Time Expression & 10 p.m.\\
Monetary Value &  \$15, \textsterling 100    \\
Percent &   30\% \\
\hline
\end{tabular}
\caption{NE Examples}
\label{table:NETypes}
\end{table}


In considering Named Entities, it is important not to confuse them with non-specific entities. % reformulate
The difference lies in the reference of the entity. A Named Entity refers to a specific, unique person/time/location, whereas
\emph{normal} entites can refer to multiple events, such as the time expression: \emph{In June}, which could refer to any year.
Also, there could be a reference to a person, as in: \emph{the prof}, which in itself is not a \namedentity.
%However, it would be a deictic reference and point back to the mention of a real \namedentity, as in the following coreference chain: \\

%\ex. $ Prof.\; Bateman \Rightarrow he \Rightarrow the \; prof$ \label{cor}\\
A Named Entity  can consist of more than one syntactic type. Syntactic types feature in syntactic rules that define how sentences can be build.
Typical high-level rules include: \\
$s \rightarrow np\quad vp $ \footnote{ A sentence s breaks down into a noun phrase np and a verb phrase vp} \\
$ vp \rightarrow v\quad (np)$ \footnote{ A verb phrase vp consists of a verb followed by an optional np} \\

Thus, sentences are often represented as syntactic trees that model the dependencies among the types.
The overall type for a Named Entity is usually a noun phrase (np), which can be both simple and complex, meaning it can consist of a only a noun phrase or 
also additional other syntactic types, such as prepositional phrases (pp), that consist of a preposition and a noun phrase. 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% - can be left out if not enough space/too detailed
It is therefore a recursive definition and in theory, noun phrases can grow infinitely big, as shown by the following recursive rules:  % 

\ex. $ np \rightarrow np \quad pp$ \\
     $ pp \rightarrow p \quad np$ 
 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 
We distinguish between pp-complements and pp-modifiers; where complements are non-optional elements of the noun phrase as they are
inherent to the meaning of the entity and modifiers simply add to the description and are optional. 
The syntactic trees in \ref{fig:bos} and \ref{fig:nirvana}  show two examples for a complex noun phrase containing a \namedentity.
\emph{N'} signals an intermediate state in the syntactic tree where modifiers can be joined.
In \ref{fig:bos}, (\emph{Bank of Scotland}), the pp \emph{of Scotland} is attached directly; it is central to the meaning, 
whereas in \ref{fig:nirvana} both \emph{Kurt Cobain} and \emph{Nirvana} constitute own concepts and are joined at a later level to express this distance.
Thus, in connection with Named Entities, we want to be able to capture types of the first example as a \namedentity, but not of the second more distantly joined 
relation.
% \begin{figure}
% \begin{enumerate}
% \item \parbox[t]{2.4in}{~\Tree  
%    [.NP [ [.DT the ] [.N\1 [.N Bank ] [.PP [.P of ] [.NP Scotland ] ] ] ] ] } \label{bos}
% \item \parbox[t]{2in}{~\Tree 
%    [.NP  [.N\1 [.N\1  \qroof{Kurt Cobain}.N  ]  [.PP [.P of ] [.N\1 [.N\1 [.N Nirvana ] ] ] ] ] ]  } \label{Nirvana}
% \end{enumerate} \label{Tree}
% \end{figure}

\begin{figure}
\Tree  
   [.NP [ [.DT the ] [.N\1 [.N Bank ] [.PP [.P of ] [.NP Scotland ] ] ] ] ] 

\caption{\namedentity: Complement Meaning}
\label{fig:bos}
\end{figure}


\begin{figure}
\Tree 
   [.NP  [.N\1 [.N\1  \qroof{Kurt Cobain}.N  ]  [.PP [.P of ] [.N\1 [.N\1 [.N Nirvana ] ] ] ] ] ]  

\caption{\namedentity: Reference Meaning}
\label{fig:nirvana}
\end{figure}

% \ex. The Bank of Scotland is in the brown building with the brown bricks on its roof... \label{PPrec}
% 
% The PP \emph{of Scotland} is semantically defining for the noun \emph{bank}, whereas \emph{with the brown bricks on its roof} 
% just adds to the description of \emph{brown building}.

\subsection{Issues in Named Entity Recognition}
Among the most common challenges for NER is the issue of ambiguity, in particular \emph{Polysemy} and \emph{Metonymy} \cite{nadeau2007survey}.
Polysemy refers to the property of a word that is a lexical representation having more than one possible meaning.
\emph{Polysemy} could become an issue for NER when this lexical representation points to two different Named Entity types.
This is quite frequent with \texttt{Person Names} and \texttt{Locations}, since many people are named after cities, such as \emph{Paris} or \emph{Georgia}. 
Often the context will not be disambiguating as in example~\ref{ex:Georgia}.

\ex. \emph{Georgia is beautiful.} \label{ex:Georgia}

\emph{Metonymy} is the relationship of a part-whole relation between two items.
It is frequently employed in literary texts and news data (which is often used in NER), where one item is substituted for another. 
Example \ref{ex:Lon} shows an instance of a \emph{whole-part}, where \emph{London} probably stands for the smaller 'item': \textbf{Goverment in London}. 

\ex. \emph{\textbf{London} decided to increase the 1200 military personnel involved in Olympic security.} \label{ex:Lon}

\subsection{Methods employed for NER}
For NER, there exist two general approaches: one is rule-based and the other statistical. 
Rule-based methods make use of the underlying rules governing a language to extract Named Entities. 
This approach, however, is rather high maintenance and requires extensive work by experts, such as computational linguists \cite{nadeau2007survey}.
The results can be quite high in precision, but lack considerably in recall. % rules are partly language-dependent

For statistical approaches, supervised-learning is the most common method applied in NER. 
There are unsupervised methods, but they cannot compete with SL applications yet.
For the shared task in 2002 \cite{tksintro}, 12 systems participated in the challenge and presented a wide variety of approaches to the Named Entity task.
The most successful system in the task, used AdaBoost applied to fixed-depth decision trees. Special attention was given to a variety of input features and the features were also
partly adapted for each language with an F-score of 81.39 on the Spanish test set and 77.05 on the Dutch one.
The second highest system \cite{Florian:2002:NER:1118853.1118863} employed three stacked
learners (transformation-based learning for obtaining base-level non-typed Named Entites, Snow for improving the quality of these entities and the forward-backward algorithm for
finding categories for the Named Entites), where the combination of these three algorithms substantially outperformed any subset of them, with an F-score of 79.05 on the Spanish test
set and 74.99 on the Dutch one.
Among the most prominent algorithms in NER are maximum entropy models, whose benefit in regard to this task became most apparent in CoNLL 2003. 
For CoNLL 2003, five of the sixteen systems used this statistical learning method and three of of those used maximum entropy models in isolation.
Maximum entropy models seem a very suitable choice for the NER task, since systems using this approach were ranked among the three highest results in 
English and the two highest results in German \cite{TjongKimSang:2003:ICS:1119176.1119195}.
The best system in both German and English \cite{Florian:2003:NER:1119176.1119201} used a combination of classifiers: a robust linear classifier, 
a maximum entropy, a transformation-based learning, and a hidden Markov model which are connected through a linear interpolation of the classifiers’ % combination scheme
class probability distribution. Particular importance is also assigned to creating a rich feature space, also including a gazetteer feature. 
This system gains a precision of 88.99\% on the English test set and  83.87\% on the German one. 

\subsection{CoNLL Data and Baseline Performance}
The training and test data for Spanish \& Dutch was taken from the CoNLL Shared Task 2002 \cite{tksintro} and 
the CoNLL Shared Task 2003 \cite{TjongKimSang:2003:ICS:1119176.1119195} provided the one for English \& German.
The ConNLL shared tasks are organised challenges, where the organisers propose a task and provide the participants with the
training and test data. 
In 2002 and 2003, the task was Named Entity Recognition. 
In both years, the baseline rate for the individual languages was produced by a system, that had a unique class in the training data. 
In case that a phrase was part of more than one entity, the system would choose the longest one \cite{TjongKimSang:2003:ICS:1119176.1119195}. 
Table \ref{table:Base} shows the baseline for the four different languages.  
The data is annotated with part-of-speech tags and named-entity tags, as well as Bio-tags that indicate the position in the Named Entity. %put in ``tokenized, chunked''
Our approach is based on \cite{strlearn}, of the group that won the challenge for Spanish and Dutch in 2002. 


\begin{table}[h!]
\scriptsize
\begin{tabular}{|l|l|l|l|}
\hline
\bf Language & \bf Precision & \bf Recall & \bf $F_1$-measure \\ \hline
Spanish &             26.27\% & 56.48\% & 35.86\%        \\
Dutch  &             64.38\%  &45.19\%    & 53.10\%  \\
English &              71.91\%& 50.90\%  & 59.61 $\pm$ 1.2\%\\
German &      31.86\%  & 28.89\% & 30.30  $\pm$ 1.3\% \\
\hline
\end{tabular}
\caption{NER Baseline}
\label{table:Base}
\end{table}

\section{Structured Prediction}
Prediction \cite{Taskar:2005:LSP:1102351.1102464} is a supervised-learning approach, where a system uses training data to learn the prediction
of unseen data. It consequently maps an input \textbf{x} to an output \textbf{y}: \\

$ x \rightarrow y $. \\

Structured Prediction and Non-structured prediction differ in the form of their output.
Non-Structured Prediction returns an atomic output: it is binary prediction for a two-class problem and one of many labels for a multiclass problem. 
Structured Prediction, on the other hand, returns the prediction for a whole sequence. 
For the sentence in \ref{PredEx1a}, it would return the corresponding label combination in example~\ref{PredEx1b}. 

\begin{figure*}[ht]

\ex. \emph{The motor company 'Ford' was founded and incorporated  by Henry Ford.} \label{PredEx1a}
%The motor company 'Ford' was founded and incorporated  by Henry Ford on June, 16th 1903.
 
\exg. \textbf{x:} The motor company ' Ford ' was founded and incorporated by Henry Ford .\\
      \textbf{y:}  O   O      O     0 ORG  O  O     O     O       O        O PER   PER  O  \label{PredEx1b} \\
\caption{Input and predicted structure for the \namedentity task.}

\end{figure*}

In comparison to other similar approaches, Structured Prediction sets itself apart through the combination of features and label interactions instead
of concentrating mainly on label interactions like HMM or a rich set of features like local classifiers. 

\section{Implementation}
In this section, we describe our implementation, the learning and decoding algorithm and the features.


\subsection{Learning and Decoding}

% problem external data / use Dbpedia - gazetteer
Our implementation for the languages English, German, Dutch and Spanish is trained and tested on the CoNLL 2003 and CoNLL 2002 data sets respectively. 

As the predicted structure, we use the labels for every token in the full sentence. If a token is not a \namedentity, it is assigned the label \Oo.

Learning is implemented using the Structured Perceptron algorithm. To avoid overfitting, we average the parameters after the last iteration \cite{collins2002discriminative}. 
For finding the best sequence of labels for a sentence, we use a modified version of the Viterbi algorithm. The algorithm finds the sequence $\hat{y}$, such that:

\[
\hat{y} = \argmax_{\mathbf{y} \in \mathcal{Y}^{n}} \sum_{i=1}^{n}\mathbf{w} \cdot \boldsymbol{f}(\mathbf{x}, i, y_{i-1}, y_{i})
\]

This sequence can be computed efficiently in $ O( N^2 |\mathbf{x}| ) $ via dynamic programming. 
As a first step, the trellis has to be constructed, then we have to find the $ a \in \mathcal{Y}$ in the 
last column of the trellis with maximal score $\delta_n(a)$. From this, the sequence can be recovered via back-tracking trough the trellis. 
The score of the best sequence ending in $a$ is:

\[
\delta_i(a) = \max_{\mathbf{y} \in \mathcal{Y}^{n}, y_n = a} \sum_{j=1}^{n}{\mathbf{w} \cdot \boldsymbol{f}(\mathbf{x}, j, y_{j-1}, y_{j})}
\]

\noindent This function $\delta_i(a)$ can be defined recursively as:

\begin{align*}
\delta_1(a) &= \mathbf{w} \cdot \boldsymbol{f}(\mathbf{x}, 1, \emptyset, a) \\
\delta_i(a) &= \max_{b \in \mathcal{Y}} \delta_{i-1}(b) + \mathbf{w} \cdot \boldsymbol{f}(\mathbf{x}, i, b, a) \\
\end{align*}


\subsection{Features}
The features employed in the system can be divided into three categories: node, label and gazetteer features. 
We describe each of the three groups in the following.

\paragraph*{Node features}
Node features are features that only depend on the current token (node): the \emph{Token}, suffix and prefix, capitalisation.

Table~\ref{table:node} shows the various node features with an example respectively.

\begin{table}[h!]
\begin{tabular}{| l | l |}
\hline
\bf Feature & \bf Example \\
\hline
Token &  \textbf{``Amsterdam''}\\
Suffix& Amster\textbf{dam}\\
Prefix&  \textbf{San} Sebastian\\
Captitalized& \textbf{B}enetton\\
Number Pattern & \\
UPPERCASE &  \textbf{BENETTON}\\
POS-tag &  Benetton \textbf{NNP}   \\
Lemma &  produced $\Rightarrow$ produce \\
\hline
\end{tabular}
\caption{Node Features}
\label{table:node}
\end{table}

\paragraph*{Label Interaction Features}
These features register which label has been assigned to the previous token and takes into account the most likely sequence. 
Thus, for the sequence: ``Jack London went to New York'' the NE tag combination of \emph{PER} and \emph{PER} in example~\ref{seq1}is more likely 
than\emph{PER} and \emph{LOC} as presented in \ref{seq2}.
.
%current token and last label for prepositions or possessive ’s

\exg. Jack London went to New York .\\
      PER   PER   O    O  LOC LOC  O \\\label{seq1}

\exg. Jack London went to New York . \\ 
      PER  LOC    O    O  LOC LOC O \\\label{seq2}
    

\paragraph*{Gazetteer Features}
A technique commonly used in Named Entity Recognition is the use of external lists of names of a specific type (gazetteer). 
In order to create gazetteer lists for the more common named entities, we use a \emph{SPARQL} query to retrieve entries from \emph{DBpedia} for all required languages.

\begin{figure}
\begin{verbatim}
PREFIX dbp: <http://dbpedia.org/ontology/>
SELECT ?label
WHERE {
    ?entity rdf:type dbp:CLASS.
    ?entity rdfs:label ?label.
    FILTER (lang(?label) = LANGUAGE)
}
\end{verbatim}
\caption{SPARQL query for retrieving names of the class \texttt{CLASS} from DBpedia.}
\end{figure}





\section{Experiments and Evaluation}
In the following section we present our experiments and the evalution of our system.
\subsection*{Experiments}


\subsection*{Evaluation}
For the evaluation of the system we used \emph{Precision} and \emph{Recall} as shown in \ref{Precision} and \ref{Recall} respectively.
Precision is the percentage of Named Entities found by the system that are correct. Recall represents the the percentage of 
named entites present in the corpus that were found by the system. 
A named entity is only deemed correct if it is an exact match for the corresponding data file.
The general formula for the \emph{F-Score} is shown in \ref{Fscore}. Since we rate both \emph{Precision} and \emph{Recall} evenly, we 
use the harmonic mean as shown in \ref{F1}.


\ex. \emph{Precision} = $ \frac{gold\; tag \bigcap predicted}{predicted}$ \label{Precision}\\


\ex. \emph{Recall} = $ \frac{gold \;tag \bigcap predicted}{gold\; tag}$ \label{Recall}\\


\ex. $F_{\beta}$ = $ (1+\beta^2)*\frac{precision *recall}{\beta^2* precision + recall}$ \label{Fscore}\\

\ex. $F_1$ = $ 2*\frac{precision *recall}{precision + recall}$ \label{F1}\\




\begin{table*}[t]
\centering
\begin{tabular}{| l | l l l| l l l |}

\hline
\bf Language & \multicolumn{3}{c|}{ \bf Test A}&\multicolumn{3}{c|}{ \bf Test B}\\
             & Precision & Recall & $F_1$ & Precision & Recall & $F_1$ \\ \hline
Spanish &       &          &     &          &               & \\
Dutch  &         &          &     &          &               &   \\
English &        &          &     &          &               &       \\
German &      &          &       &          &             & \\
\hline
\end{tabular}
\caption{NER Structured Prediction Results }
\label{table:Results}
\end{table*}

\section{Discussion of Results}

\subsection*{Some Challenges} % maybe in previous section


\section{Future Work \& Conclusion}

This paper presented a system for \namedentity Recognition using Structured Prediction. 


\bibliographystyle{acl}
\bibliography{paper}

\end{document}
